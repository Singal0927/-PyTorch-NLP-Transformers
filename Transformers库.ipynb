{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COvqxN19nIyW"
      },
      "source": [
        "## Transformers库\n",
        "情感分析教程来源于 https://huggingface.co/docs/transformers/quicktour 。\n",
        "### 一、使用pipeline\n",
        "第一次使用pipeline载入任务时，如\"sentiment-analysis\"，预训练模型和分词器会被自动下载，pipeline将预训练模型与分词器结合起来，需要导入两个类，分别是AutoTokenizer——用于下载分词器，以及AutoModelForSequenceClassification——用于下载模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8mKl46s9nIyZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification,pipeline\n",
        "from transformers import BertTokenizer,BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hTgqEMfnIya"
      },
      "source": [
        "使用from_pretrained方法来加载训练过的模型（由transformers社区提供）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eamTTFw1nIya"
      },
      "outputs": [],
      "source": [
        "model_name='distilbert-base-uncased-finetuned-sst-2-english'\n",
        "model=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "classifier=pipeline('sentiment-analysis',model=model,tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W5xXmlrnIyb",
        "outputId": "20273f88-99b8-4e99-d566-4f03a0416112"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998652935028076},\n",
              " {'label': 'NEGATIVE', 'score': 0.9987472295761108}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results=classifier(['Happy!','I hate you!'])\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWnVvFVpnIyb"
      },
      "source": [
        "### 二、使用Tokenizer、Model、Configuration\n",
        "因为分词的方法众多，故Model与Tokennizer所使用的模型必须一致。Tokenizer主要包含两个功能：\n",
        "1. 依据某种分词规则，将给定的一句话划分为单词（token），分词规则由使用的模型决定；\n",
        "2. 将token转换成数字（词向量），并构建张量，主要通过vocab来实现——包含在from_pretrained方法中。\n",
        "\n",
        "Model接收Tokenizer的输出，Configuration用于自定义超参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s85uyQffnIyc"
      },
      "outputs": [],
      "source": [
        "inputs=tokenizer('Hello world!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y47IOh1vnIyc"
      },
      "source": [
        "tokenizer返回了一个字典，input_ids表示每个token在vocab中所对应的位置，attention_mask能够帮助模型更好的理解句子的含义。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARMz9KXxnIyd",
        "outputId": "2824f726-fa38-4de3-ecb7-cdfef8eab047"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7592, 2088, 999, 102], 'attention_mask': [1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSNBbG3LnIyd"
      },
      "source": [
        "也可以向tokenizer传入由句子组成的list，并设置多种参数：\n",
        "1. padding=True，对齐词向量的长度；\n",
        "2. trunction=True，规定词向量的最大长度，需同时设置max_length；\n",
        "3. return_tensors='pt'，返回pytorch的数据类型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F03HBuC0nIyd",
        "outputId": "2c73c173-8ba0-4608-df0b-92c9bf23568b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_items([('input_ids', tensor([[  101,  2006,  2028,  2217,  1997,  1996,  3675,  2090,  3607,  1998,\n",
              "          5924,  1010,  2062,  2084,  2531,  1010,  2199,  1997,  4924,  1005,\n",
              "          1055,  3629,  2024,  3742,  2098,  1012,   102,     0],\n",
              "        [  101,  2006,  1996,  2670,  2217,  1010,  5190,  1997,  5969,  4480,\n",
              "          2024,  2893,  2510,  2731,  2000, 16360,  2884,  2054,  2530,  4454,\n",
              "          2758,  2003,  1037,  2825,  2845,  2886,  1012,   102]])), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1]]))])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch=tokenizer(\n",
        "    [\"On one side of the border between Russia and Ukraine, more than 100,000 of Moscow's troops are massed.\",'On the southern side, thousands of Ukrainian citizens are getting military training to repel what western intelligence says is a possible Russian attack.'],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "batch.items()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TCD2tm4nIye"
      },
      "source": [
        "接下来便是将词向量送入model中，对于pytorch模型，需要解包字典，即添加**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4i2GvfMnIye",
        "outputId": "87628f75-f568-42b4-f496-a76a9e0a174b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('logits', tensor([[-0.6354,  0.7732],\n",
              "                                   [ 1.8996, -1.6244]], grad_fn=<AddmmBackward0>))])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output=model(**batch)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BraybdZKnIyf"
      },
      "source": [
        "所有的Transformers模型的返回值均未施加激活函数，故需手动施加。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtbLlDjxnIyf",
        "outputId": "7d90f105-e1be-44a5-bb0b-1883ebdaf563"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.1965, 0.8035],\n",
              "        [0.9714, 0.0286]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "activation=F.softmax(output.logits,dim=1)\n",
        "activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2p42zAnIyf"
      },
      "source": [
        "对于监督学习，模型能够计算出损失值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz2mauM3nIyg",
        "outputId": "e513b69e-491a-46c4-a722-e61c944c97e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('loss', tensor(0.1239, grad_fn=<NllLossBackward0>)),\n",
              "                          ('logits', tensor([[-0.6354,  0.7732],\n",
              "                                   [ 1.8996, -1.6244]], grad_fn=<AddmmBackward0>))])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "output=model(**batch,labels=torch.tensor([1,0]))\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2R2_E-nnIyg"
      },
      "source": [
        "### 三、文本分类实战"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SeC_Eey9nIyg"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset#Dataset与DataLoader配合使用，用Dataset读取数据，再用DataLoader自定义batch\n",
        "from transformers import BertTokenizer,BertForSequenceClassification,BertConfig\n",
        "from transformers import AdamW\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qzF4KDGbnIyh"
      },
      "outputs": [],
      "source": [
        "dropout=0.1\n",
        "num_labels=2\n",
        "lr=1e-5\n",
        "weight_decay=1e-2#正则项系数之一，防止过拟合\n",
        "epochs=2\n",
        "batch_size=32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPRdf4lsnIyh"
      },
      "source": [
        "首先简单查看数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TFxs_DcGnIyh"
      },
      "outputs": [],
      "source": [
        "temp=pd.read_csv('/content/drive/My Drive/Colab Notebooks/sentiment/sentiment.train.data',sep='\\t',names=['text','label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DCaA3z-TnIyi",
        "outputId": "2ffbb030-38d4-4a78-c41a-8968ce03a4f0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bf9f18e7-5d6f-4638-8097-0923d04dcf52\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>贝贝好爱干净 每天出门都要洗澡 还喜欢喝蒙牛 不喜欢蹲地方 喜欢坐凳子上还喜欢和我坐在一起~</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>感觉好像是文科生看一本《高等数学》的教材一样，流水账一般，只是背景很好罢了，选择在这样一个竞...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>很安静,隔音设施不错.服务员态度很好,下次还会选这里</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 感觉外观还可以，符合我的要求，体积虽不算小，但比它大的翻盖手机还是很多的。2 比一张IC...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>收到后，包装完好。笔记本封条完好。 性价比很高，DVD驱动盘包含VISTA所有必备的驱动，方便。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf9f18e7-5d6f-4638-8097-0923d04dcf52')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf9f18e7-5d6f-4638-8097-0923d04dcf52 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf9f18e7-5d6f-4638-8097-0923d04dcf52');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text  label\n",
              "0     贝贝好爱干净 每天出门都要洗澡 还喜欢喝蒙牛 不喜欢蹲地方 喜欢坐凳子上还喜欢和我坐在一起~      1\n",
              "1  感觉好像是文科生看一本《高等数学》的教材一样，流水账一般，只是背景很好罢了，选择在这样一个竞...      0\n",
              "2                         很安静,隔音设施不错.服务员态度很好,下次还会选这里      1\n",
              "3  1 感觉外观还可以，符合我的要求，体积虽不算小，但比它大的翻盖手机还是很多的。2 比一张IC...      1\n",
              "4   收到后，包装完好。笔记本封条完好。 性价比很高，DVD驱动盘包含VISTA所有必备的驱动，方便。      1"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Rgs8xyLDnIyi"
      },
      "outputs": [],
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    #所定义的三个函数是必须的\n",
        "    def __init__(self,path):\n",
        "        self.data=pd.read_csv(path,sep='\\t',names=['text','label'])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self,index):#取出指定元素\n",
        "        text=self.data.loc[index,'text']\n",
        "        label=self.data.loc[index,'label']\n",
        "        sample={\"text\":text,'label':label}\n",
        "        return sample\n",
        "\n",
        "train=SentimentDataset('/content/drive/My Drive/Colab Notebooks/sentiment/sentiment.train.data')\n",
        "valid=SentimentDataset('/content/drive/My Drive/Colab Notebooks/sentiment/sentiment.valid.data')\n",
        "\n",
        "train=DataLoader(train,batch_size=batch_size,shuffle=True)\n",
        "valid=DataLoader(valid,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "def getSize(dataloader):\n",
        "    labels=next(iter(dataloader))['label']\n",
        "    features=next(iter(dataloader))['text']\n",
        "    print(f'labels size:{len(labels)},features size:{len(features)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "layD2qMlnIyi",
        "outputId": "523ec62c-daab-4349-c29c-b146fbf3d8f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['呜呜呜~~~我的蒙牛优益c啊！！好不容易买一瓶。给我撒了一半！', '我买了一套，是冲着作者买的。没什么内容和深度。', '宝贝一如既往的好，这已经是第三台了，物流也很快', '国航就爱配蒙牛酸~奶~ 如果中国有几千张这么“刺儿头”的脸，制度不灵的情况下，民航业也会改善很多......干，什么业又不是呢？ 国航搞这些小动作只能忽悠傻逼，可他是罗胖子啊，这张脸以后估计所有航空公司都会谨记了', '无聊,无病呻吟,照她的说法,女人最好赶快找个有钱人嫁了,还得赶在20几岁的时候,晕死,自己认为自己是贵族,别人就当你是贵族,有病吧,棒子就是会意淫!']\n",
            "tensor([0, 1, 0, 1, 0])\n"
          ]
        }
      ],
      "source": [
        "print(next(iter(train))['text'][:5])\n",
        "print(next(iter(train))['label'][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w86gh8JonIyj",
        "outputId": "8634263d-974c-4e43-c46e-855e19c4976d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "527"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train)#共有527个batch，说明样本总数为527*32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chJ-j-ORnIyj",
        "outputId": "c30aef62-f391-4d7f-c2f2-a4f22318cbac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "labels size:32,features size:32\n"
          ]
        }
      ],
      "source": [
        "getSize(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyhFp7LLnIyj",
        "outputId": "1c2db43d-0362-4cd6-eecb-dc9f47d03ecd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer=BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
        "model=BertForSequenceClassification.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
        "config=BertConfig.from_pretrained('hfl/chinese-roberta-wwm-ext',num_labels=num_labels,hidden_dropout=dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQLCJMlhnIyk",
        "outputId": "d17b4228-1927-4838-c3e6-cdd31147e8ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "optimizer=AdamW(model.parameters(),lr=lr)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "from transformers import get_scheduler\n",
        "num_training_steps=epochs*len(train)\n",
        "\n",
        "lr_scheduler=get_scheduler('linear',optimizer=optimizer,num_warmup_steps=0,num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "j6Mh8e2OnIyk"
      },
      "outputs": [],
      "source": [
        "def train_loop(data,tokenizer,model,optimizer,criterion,device):\n",
        "    loop_acc=0\n",
        "    loop_loss=0\n",
        "    model.train()\n",
        "    for index,batch in enumerate(data):\n",
        "        #因为原始数据中包括\"text\"、\"label\"\n",
        "        label=batch['label']\n",
        "        text=batch['text']\n",
        "        label=label.to(device)\n",
        "        #分词\n",
        "        token_text=tokenizer(text,max_length=100,add_special_tokens=True,truncation=True,padding=True,return_tensors='pt')\n",
        "        token_text=token_text.to(device)\n",
        "        #传入模型\n",
        "        output=model(**token_text,labels=label)#返回的第一个值是loss，第二个值是batch_size个logits\n",
        "        prob=output[1]\n",
        "        pred=prob.argmax(dim=1)\n",
        "        #计算loss\n",
        "        loss=criterion(prob,label)#交叉熵损失函数要求第一个输入值为预测概率矩阵，而非label\n",
        "        loop_loss+=loss\n",
        "        acc=(pred==label).sum()\n",
        "        loop_acc+=acc\n",
        "        #反向传播\n",
        "        loss.backward()\n",
        "        #更新权重和学习率\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        #梯度清零\n",
        "        optimizer.zero_grad()\n",
        "        if index == 300:\n",
        "          break\n",
        "        if index % 50 == 0:\n",
        "            print(f'前{index+1}个batch的累计损失值为：{loop_loss}，准确率为{loop_acc/(index+1)/batch_size}')\n",
        "\n",
        "def test_loop(data,tokenizer,model,optimizer,criterion,device):\n",
        "    loop_acc=0\n",
        "    loop_loss=0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for index,batch in enumerate(data):\n",
        "            label=batch['label']\n",
        "            text=batch['text']\n",
        "            label=label.to(device)\n",
        "            #分词\n",
        "            token_text=tokenizer(text,max_length=100,add_special_tokens=True,truncation=True,padding=True,return_tensors='pt').to(device)\n",
        "            #传入模型\n",
        "            output=model(**token_text,labels=label)\n",
        "            prob=output[1]\n",
        "            pred=prob.argmax(dim=1)\n",
        "            #计算loss\n",
        "            loss=criterion(prob,label)\n",
        "            loop_loss+=loss\n",
        "            acc=(pred==label).sum()\n",
        "            loop_acc+=acc\n",
        "\n",
        "            if index % 50 == 0:\n",
        "                print(f'前{index+1}个batch的累计损失值为：{loop_loss}，准确率为{loop_acc/(index+1)/batch_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bfvqU_NnIyl",
        "outputId": "35339a1c-11e7-41f8-8831-bb72b180b251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "第1次结果：\n",
            "前1个batch的累计损失值为：0.7147435545921326，准确率为0.5\n",
            "前51个batch的累计损失值为：27.39373779296875，准确率为0.7334558963775635\n",
            "前101个batch的累计损失值为：40.4895133972168，准确率为0.8189975023269653\n",
            "前151个batch的累计损失值为：53.313682556152344，准确率为0.8485099077224731\n",
            "前201个batch的累计损失值为：64.30728912353516，准确率为0.8658270835876465\n",
            "前251个batch的累计损失值为：75.95376586914062，准确率为0.8751245141029358\n",
            "前1个batch的累计损失值为：0.2706177532672882，准确率为0.90625\n",
            "前51个batch的累计损失值为：10.098142623901367，准确率为0.930759847164154\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "第2次结果：\n",
            "前1个batch的累计损失值为：0.24100948870182037，准确率为0.875\n",
            "前51个batch的累计损失值为：9.166712760925293，准确率为0.9350490570068359\n",
            "前101个batch的累计损失值为：17.778465270996094，准确率为0.9396658539772034\n",
            "前151个batch的累计损失值为：25.911706924438477，准确率为0.9395695328712463\n",
            "前201个batch的累计损失值为：33.16047668457031，准确率为0.9426305890083313\n",
            "前251个batch的累计损失值为：41.41641616821289，准确率为0.942729115486145\n",
            "前1个batch的累计损失值为：0.34892165660858154，准确率为0.875\n",
            "前51个batch的累计损失值为：8.623793601989746，准确率为0.9375000596046448\n"
          ]
        }
      ],
      "source": [
        "for i in range(epochs):\n",
        "    print('\\n','-'*100,sep='')\n",
        "    print(f'第{i+1}次结果：')\n",
        "    train_loop(train,tokenizer,model,optimizer,criterion,device)\n",
        "    test_loop(valid,tokenizer,model,optimizer,criterion,device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Transformers库.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "da5d3c13d5aa829161cf843792884de182f91ab2e540693bb706e0e23cabc9e9"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('py38': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
