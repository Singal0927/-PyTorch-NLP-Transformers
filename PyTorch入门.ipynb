{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/singal/miniforge3/envs/py38/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /tmp/pip-req-build-4gogm3_o/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_data=datasets.FashionMNIST(\n",
    "    root='data',#表示数据存储路径\n",
    "    train=True,\n",
    "    download=True,#若root中不包含数据，则从网络下载\n",
    "    transform=ToTensor()#表示将特征转换为tensors\n",
    ")\n",
    "\n",
    "test_data=datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取数据之后，需要分批次地将数据传入模型中，以降低过拟合，DataLoader能够满足我们的需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader=DataLoader(train_data,batch_size=64,shuffle=True)\n",
    "test_dataloader=DataLoader(test_data,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': Dataset FashionMNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " 'num_workers': 0,\n",
       " 'prefetch_factor': 2,\n",
       " 'pin_memory': False,\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None,\n",
       " '_DataLoader__multiprocessing_context': None,\n",
       " '_dataset_kind': 0,\n",
       " 'batch_size': 64,\n",
       " 'drop_last': False,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x13f43dac0>,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x13f43d3a0>,\n",
       " 'generator': None,\n",
       " 'collate_fn': <function torch.utils.data._utils.collate.default_collate(batch)>,\n",
       " 'persistent_workers': False,\n",
       " '_DataLoader__initialized': True,\n",
       " '_IterableDataset_len_called': None,\n",
       " '_iterator': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0039, 0.0000,  ..., 0.0118, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]),\n",
       " tensor([9, 6, 9, 4, 1, 0, 2, 0, 2, 8, 3, 3, 1, 0, 8, 9, 0, 2, 9, 7, 9, 8, 7, 7,\n",
       "         5, 4, 9, 8, 7, 4, 4, 1, 3, 1, 4, 1, 3, 6, 4, 6, 2, 8, 2, 1, 6, 9, 0, 7,\n",
       "         6, 2, 0, 2, 7, 9, 3, 3, 3, 5, 5, 9, 7, 5, 1, 3])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、探究dataloader到底是什么\n",
    "dataloader中共有（数据总数/batch_size）个样本，每个样本含有batch_size个数据。\n",
    "\n",
    "dataloader.dataset中存储的是dataloader所使用的数据集，本段代码中即为上文的train_data。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features,train_labels=next(iter(train_dataloader))#next与iter搭配使用\n",
    "train_features.size()#64对应上文的一次取64个样本，可视为样本的序号？，1、28、28为图像的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(),y.size()#每个样本含有batch_size个数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=next(iter(train_dataloader.dataset))#dataloader.dataset中存储的是dataloader所使用的数据集，本段代码中即为上文的train_data\n",
    "x.size(),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 60000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader),len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据转换\n",
    "数据通常需要经过一系列处理，才能作为能被机器学习模型识别的数据，从而传入模型中，这就是transform的用武之地。为了训练模型（估计参数），特征数据需被转换为标准张量，标签也需要被转换为One-Hot编码，实现上述功能可以借助ToTensor和Lambda。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor,Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y:torch.size(10).scatter_(dim=0,index=torch.tensor(y),value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、构造神经网络模型\n",
    "### （一）概述\n",
    "神经网络由多个层（layers），又名模块（modules）组成。torch.nn作为一个命名空间，提供多样化的“砖石”来帮助使用者构建神经网络。每一个模块均是nn.Moduel的子类。一个神经网络本身就是一个模块，且由众多子模块构成，即“嵌入式”结构——nested structure。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'#截止2021年11月19日，m1芯片仍不支持cuda\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过继承nn.Module来定义神经网络，在__init__中初始化神经网络层（layers）。每一个nn.Module的子类均使用forward方法对输入数据进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()#PyTorch的指定语句\n",
    "        #以下代码用来定义层：\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "            nn.Linear(28*28,512),#特征维度为28*28，512代表神经元个数\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)#10代表输出数据的维度，因为MNIST数据集的标签种类为10个\n",
    "        )\n",
    "    def forward(self,x):#用以对输入数据进行操作\n",
    "        x=self.flatten(x)#操作之前，需将数据“变平”\n",
    "        logits=self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=NeuralNetwork().to(device)#字面理解，将实例化后的对象“送入”（to）device中进行操作\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0049, -0.0056,  0.0767,  0.0209, -0.0046,  0.0901, -0.0629,  0.1156,\n",
       "          0.0387, -0.1008]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.rand(1,28,28)#0-1均匀分布\n",
    "logits=model(X)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一个线性组合层根据逻辑损失函数，返回了负值。将该负值传入nn.Softmax的某个实例中，即被重新转化为（scale）在[0,1]上的取值。dim参数表示沿着哪条轴/维度的数据之和为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0977, 0.0976, 0.1060, 0.1003, 0.0977, 0.1074, 0.0922, 0.1102, 0.1021,\n",
       "         0.0888]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pro=nn.Softmax(dim=1)(logits)\n",
    "pred_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=pred_pro.argmax(1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:linear_relu_stack.0.weight | Size:torch.Size([512, 784]) | \n",
      "Values:tensor([[ 3.2909e-02,  2.6293e-02,  2.0720e-03,  ...,  2.4925e-02,\n",
      "         -5.3411e-04, -1.5061e-05],\n",
      "        [ 1.0271e-03,  2.0648e-02, -3.1945e-02,  ...,  1.6777e-02,\n",
      "         -1.1508e-02,  1.3032e-02]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "layer:linear_relu_stack.0.bias | Size:torch.Size([512]) | \n",
      "Values:tensor([-0.0252,  0.0019], grad_fn=<SliceBackward0>)\n",
      "\n",
      "layer:linear_relu_stack.2.weight | Size:torch.Size([512, 512]) | \n",
      "Values:tensor([[ 0.0262,  0.0050, -0.0305,  ..., -0.0315, -0.0395, -0.0084],\n",
      "        [-0.0329,  0.0189, -0.0058,  ..., -0.0358,  0.0282, -0.0030]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "layer:linear_relu_stack.2.bias | Size:torch.Size([512]) | \n",
      "Values:tensor([-0.0368,  0.0178], grad_fn=<SliceBackward0>)\n",
      "\n",
      "layer:linear_relu_stack.4.weight | Size:torch.Size([10, 512]) | \n",
      "Values:tensor([[-0.0228,  0.0092, -0.0194,  ...,  0.0416, -0.0203,  0.0415],\n",
      "        [-0.0191, -0.0406, -0.0164,  ..., -0.0067,  0.0211, -0.0034]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "layer:linear_relu_stack.4.bias | Size:torch.Size([10]) | \n",
      "Values:tensor([0.0111, 0.0343], grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'layer:{name} | Size:{param.size()} | \\nValues:{param[:2]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （二）模块中的各种层layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image=torch.rand(3,28,28)#生成3个28*28的图片数据作为演示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Flatten\n",
    "将每个样本的所有特征“铺平”，如某个样本是10*10维矩阵，则转换为100维向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 784])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten=nn.Flatten()\n",
    "flat_image=flatten(input_image)\n",
    "flat_image.size()#28×28等于784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.linear\n",
    "通过权重和偏置，将数据线性组合。即$$xw+b$$既可降维，也可升维，取决于参数的维度。\n",
    "#### nn.ReLU\n",
    "线性修正单元作为非线性激活函数，在输入数据和输出数据之间建立复杂的映射关系。需传入经过linear层的数据，可帮助神经网络习得多种多样的“非寻常特征”。\n",
    "#### nn.Sequential\n",
    "将Sequential视为一个容器，里面放置了各种层。\n",
    "#### nn.Softmax\n",
    "将损失值转化为[0,1]上的取值。\n",
    "\n",
    "## 四、最优化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "batch_size=64\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成超参数的定义后，我们应使用最优化循环（optimization loop）来训练并最优化所设置的模型。最优化循环的单次迭代被称为epoch，包含：\n",
    "1. 训练循环：利用数据集对参数迭代求解，并不断逼近最优参数；\n",
    "2. 验证循环：也称测试循环，利用测试集数据，检验模型的预测效果是否得到改善。\n",
    "\n",
    "最优化被分解为以下三个步骤：\n",
    "1. 调用optimizer.zero_grad()函数来重新设置所有参数的梯度向量为0。这是因为梯度向量默认累加，需防止每次迭代后梯度向量的重复求和。\n",
    "2. 调用loss.backward()函数来反向传播损失值，并存储损失值关于每个参数的梯度向量；\n",
    "3. 在获得所有参数的梯度向量后，调用optimizer.step()函数进行梯度下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader,model,loss_fn,optimizer):#分批训练\n",
    "    size=len(dataloader.dataset)\n",
    "    for batch,(X,y) in enumerate(dataloader):#该循环共执行（样本总数/batch_size）次\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred,y)\n",
    "        \n",
    "        optimizer.zero_grad()#最优化步骤1\n",
    "        loss.backward()#最优化步骤2\n",
    "        optimizer.step()#最优化步骤3\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss,current=loss.item(),batch*len(y)#也可以使用len(X)，二者长度均为batch_size\n",
    "            print(f'loss:{loss}   [{current}/{size}]')\n",
    "\n",
    "def test_loop(dataloader,model,loss_fn):\n",
    "    test_loss,correct=0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:#该循环共执行（样本总数/batch_size）次\n",
    "            pred=model(X)#X包含batch_size个数据\n",
    "            test_loss += loss_fn(pred,y).item()#每执行一次，表示将batch_size个数据的损失值加总\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()#每执行一次，表示计算batch_size个数据的正确预测个数\n",
    "    test_loss/=len(dataloader)#表示每batch_size个数据的loss，len(dataloader)返回的是所取的batch个数，本案例中为157，乘64之后为10048，近似等于10000\n",
    "    correct/=len(dataloader.dataset)\n",
    "    print(f'Test Error:\\nAccuracy:{100*correct}%, Avg loss:{test_loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-----------\n",
      "loss:2.29296612739563   [0/60000]\n",
      "loss:2.292689323425293   [6400/60000]\n",
      "loss:2.2929325103759766   [12800/60000]\n",
      "loss:2.2711169719696045   [19200/60000]\n",
      "loss:2.250555992126465   [25600/60000]\n",
      "loss:2.2411553859710693   [32000/60000]\n",
      "loss:2.2239990234375   [38400/60000]\n",
      "loss:2.2021379470825195   [44800/60000]\n",
      "loss:2.1875569820404053   [51200/60000]\n",
      "loss:2.180978298187256   [57600/60000]\n",
      "Test Error:\n",
      "Accuracy:42.74%, Avg loss:2.1713928356292143\n",
      "\n",
      "Epoch 2\n",
      "-----------\n",
      "loss:2.154694080352783   [0/60000]\n",
      "loss:2.1570074558258057   [6400/60000]\n",
      "loss:2.1610617637634277   [12800/60000]\n",
      "loss:2.1124253273010254   [19200/60000]\n",
      "loss:2.0882363319396973   [25600/60000]\n",
      "loss:2.050867795944214   [32000/60000]\n",
      "loss:2.0427889823913574   [38400/60000]\n",
      "loss:1.9929804801940918   [44800/60000]\n",
      "loss:1.9558532238006592   [51200/60000]\n",
      "loss:1.961291790008545   [57600/60000]\n",
      "Test Error:\n",
      "Accuracy:58.08%, Avg loss:1.9254147376224493\n",
      "\n",
      "Epoch 3\n",
      "-----------\n",
      "loss:1.9166574478149414   [0/60000]\n",
      "loss:1.884014368057251   [6400/60000]\n",
      "loss:1.876522421836853   [12800/60000]\n",
      "loss:1.7904516458511353   [19200/60000]\n",
      "loss:1.7792859077453613   [25600/60000]\n",
      "loss:1.7304247617721558   [32000/60000]\n",
      "loss:1.6913632154464722   [38400/60000]\n",
      "loss:1.6310402154922485   [44800/60000]\n",
      "loss:1.6402891874313354   [51200/60000]\n",
      "loss:1.546755313873291   [57600/60000]\n",
      "Test Error:\n",
      "Accuracy:62.55%, Avg loss:1.5549241699230898\n",
      "\n",
      "Epoch 4\n",
      "-----------\n",
      "loss:1.6190659999847412   [0/60000]\n",
      "loss:1.6211943626403809   [6400/60000]\n",
      "loss:1.5635790824890137   [12800/60000]\n",
      "loss:1.436381459236145   [19200/60000]\n",
      "loss:1.4804092645645142   [25600/60000]\n",
      "loss:1.2994390726089478   [32000/60000]\n",
      "loss:1.353446364402771   [38400/60000]\n",
      "loss:1.3124547004699707   [44800/60000]\n",
      "loss:1.3064430952072144   [51200/60000]\n",
      "loss:1.3352845907211304   [57600/60000]\n",
      "Test Error:\n",
      "Accuracy:63.800000000000004%, Avg loss:1.2675707173195614\n",
      "\n",
      "Epoch 5\n",
      "-----------\n",
      "loss:1.3178989887237549   [0/60000]\n",
      "loss:1.1881211996078491   [6400/60000]\n",
      "loss:1.1352163553237915   [12800/60000]\n",
      "loss:1.1100006103515625   [19200/60000]\n",
      "loss:1.127898097038269   [25600/60000]\n",
      "loss:1.122830867767334   [32000/60000]\n",
      "loss:1.1296480894088745   [38400/60000]\n",
      "loss:1.2864540815353394   [44800/60000]\n",
      "loss:1.0770381689071655   [51200/60000]\n",
      "loss:1.1372953653335571   [57600/60000]\n",
      "Test Error:\n",
      "Accuracy:64.9%, Avg loss:1.0927629041823612\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "lr=1e-3\n",
    "batch_size=64\n",
    "epochs=5\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f'Epoch {i+1}\\n-----------')\n",
    "    train_loop(train_dataloader,model,loss_fn,optimizer)\n",
    "    test_loop(test_dataloader,model,loss_fn)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da5d3c13d5aa829161cf843792884de182f91ab2e540693bb706e0e23cabc9e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
