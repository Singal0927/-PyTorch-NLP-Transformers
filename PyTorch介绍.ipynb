{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、Tensor张量\n",
    "在PyTorch中，使用张量对模型的输入输出以及参数进行编码。其优点是能够调用GPU等硬件的性能，加快计算速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （一）Tensor的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[[1,2],[3,4]]\n",
    "x_list=torch.tensor(data)#列表作为参数\n",
    "x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr=np.array(data)\n",
    "x_arr=torch.tensor(arr)#array作为参数\n",
    "x_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      " tensor([[0.9881, 0.9710],\n",
      "        [0.4307, 0.5097]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_ones=torch.ones_like(x_list)#tensor作为参数，保留特性\n",
    "x_rand=torch.rand_like(x_list,dtype=float)#tensor作为参数，不保留特性（overridden）\n",
    "\n",
    "print(x_ones,'\\n',x_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8154, 0.4208, 0.6867],\n",
      "        [0.6234, 0.2411, 0.4388]]) \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape=(2,3,)\n",
    "rand_tensor=torch.rand(shape)#随机生成给定形状的tensor\n",
    "ones_tensor=torch.ones(shape)\n",
    "zeros_tensor=torch.zeros(shape)\n",
    "print(rand_tensor,'\\n',ones_tensor,'\\n',zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （二）Tensor的属性\n",
    "包括形状、数据类型和存储的硬件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor=torch.rand(3,4)\n",
    "tensor.shape,tensor.dtype,tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （三）Tensor的操作\n",
    "张量拥有超过100个定义操作，包括调换顺序、索引、切片、拼接等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8209, 0.9723, 0.5549, 0.4708, 0.8209, 0.9723, 0.5549, 0.4708],\n",
       "         [0.0248, 0.5844, 0.4228, 0.2037, 0.0248, 0.5844, 0.4228, 0.2037],\n",
       "         [0.2525, 0.7014, 0.6546, 0.7477, 0.2525, 0.7014, 0.6546, 0.7477]]),\n",
       " tensor([[0.8209, 0.9723, 0.5549, 0.4708],\n",
       "         [0.0248, 0.5844, 0.4228, 0.2037],\n",
       "         [0.2525, 0.7014, 0.6546, 0.7477],\n",
       "         [0.8209, 0.9723, 0.5549, 0.4708],\n",
       "         [0.0248, 0.5844, 0.4228, 0.2037],\n",
       "         [0.2525, 0.7014, 0.6546, 0.7477]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1=torch.cat([tensor,tensor],dim=1)#拼接\n",
    "t2=torch.cat([tensor,tensor],dim=0)\n",
    "t1,t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[6.7386e-01, 9.4545e-01, 3.0790e-01, 2.2168e-01],\n",
       "         [6.1409e-04, 3.4154e-01, 1.7875e-01, 4.1504e-02],\n",
       "         [6.3759e-02, 4.9199e-01, 4.2844e-01, 5.5903e-01]]),\n",
       " tensor([[6.7386e-01, 9.4545e-01, 3.0790e-01, 2.2168e-01],\n",
       "         [6.1409e-04, 3.4154e-01, 1.7875e-01, 4.1504e-02],\n",
       "         [6.3759e-02, 4.9199e-01, 4.2844e-01, 5.5903e-01]]),\n",
       " tensor([[0.8209, 0.9723, 0.5549, 0.4708],\n",
       "         [0.0248, 0.5844, 0.4228, 0.2037],\n",
       "         [0.2525, 0.7014, 0.6546, 0.7477]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.mul(tensor),tensor*tensor,tensor#张量的对应元素相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.1489, 0.9191, 1.6045],\n",
       "         [0.9191, 0.5624, 0.8452],\n",
       "         [1.6045, 0.8452, 1.5432]]),\n",
       " tensor([[2.1489, 0.9191, 1.6045],\n",
       "         [0.9191, 0.5624, 0.8452],\n",
       "         [1.6045, 0.8452, 1.5432]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.matmul(tensor.T),tensor@tensor.T#矩阵乘法 matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "操作函数后加入下划线\"_\"后，即表示修改原变量。具有“替换”功能的操作函数的使用会占用一些内存，故不鼓励使用。\n",
    "## （四）Tensor与array之间的相互转换\n",
    "运行在CPU上的tensor与array占有共同的内存空间，一方的改变会同时引起另一方改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8209, 0.9723, 0.5549, 0.4708],\n",
       "         [0.0248, 0.5844, 0.4228, 0.2037],\n",
       "         [0.2525, 0.7014, 0.6546, 0.7477]]),\n",
       " array([[0.8208881 , 0.97234315, 0.55489105, 0.47082973],\n",
       "        [0.02478093, 0.5844118 , 0.4227836 , 0.2037245 ],\n",
       "        [0.25250524, 0.7014173 , 0.65455616, 0.7476852 ]], dtype=float32),\n",
       " tensor([[0.8209, 0.9723, 0.5549, 0.4708],\n",
       "         [0.0248, 0.5844, 0.4228, 0.2037],\n",
       "         [0.2525, 0.7014, 0.6546, 0.7477]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array=tensor.numpy()#tensor变array\n",
    "tensor1=torch.from_numpy(array)#array变tensor\n",
    "tensor,array,tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.8209, 1.9723, 1.5549, 1.4708],\n",
       "         [1.0248, 1.5844, 1.4228, 1.2037],\n",
       "         [1.2525, 1.7014, 1.6546, 1.7477]]),\n",
       " array([[1.820888 , 1.9723432, 1.5548911, 1.4708297],\n",
       "        [1.024781 , 1.5844119, 1.4227836, 1.2037245],\n",
       "        [1.2525053, 1.7014173, 1.6545562, 1.7476852]], dtype=float32),\n",
       " tensor([[1.8209, 1.9723, 1.5549, 1.4708],\n",
       "         [1.0248, 1.5844, 1.4228, 1.2037],\n",
       "         [1.2525, 1.7014, 1.6546, 1.7477]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.add_(1)#双方均发生改变\n",
    "tensor,array,tensor1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、Autograd 自动求导\n",
    "torch.autograd是PyTorch的自动求导引擎。先以单一训练步骤为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,torchvision\n",
    "model=torchvision.models.resnet18(pretrained=True)\n",
    "data=torch.rand(1,3,64,64)#一个三通道样本图片，大小为64*64\n",
    "labels=torch.rand(1,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （一）Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （二）Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=(prediction-labels).sum()#计算损失值\n",
    "loss.backward()#autograd计算梯度向量并储存在每个参数的.grad属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim=torch.optim.SGD(model.parameters(),lr=1e-2,momentum=0.9)#加载某个最优器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()#梯度下降，更新权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （三）Autograd 微分机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8., 45.], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([2.,3.],requires_grad=True)\n",
    "b=torch.tensor([4.,6.],requires_grad=True)\n",
    "Q=3*a**3-b**2\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为Q是个向量，所有需将其转变为标量后再调用backward方法。根据微分法则，求解得a的梯度向量应为(36,81)，b的梯度向量应为(-8,-12)，正好与Autograd的结果一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([36., 81.]), tensor([ -8., -12.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.sum().backward()\n",
    "a.grad,b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者在backward函数中传入一张量$\\overrightarrow{v}$，该张量的维度与Q一致，元素均为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([36., 81.]), tensor([ -8., -12.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([2.,3.],requires_grad=True)\n",
    "b=torch.tensor([4.,6.],requires_grad=True)\n",
    "Q=3*a**3-b**2\n",
    "\n",
    "external_grad=torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "a.grad,b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总的来说，autograd自动求导引擎用以计算Vector-Jacobian $J^T\\overrightarrow{v}$ ，其中$\\overrightarrow{v}$是标量函数$l=g(\\overrightarrow{y})$的梯度向量。根据链式法则，$J^T\\overrightarrow{v}$即为$l$关于$\\overrightarrow{x}$的梯度向量。\n",
    "$$\n",
    "J=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1}&\\cdots&\\frac{\\partial y_1}{\\partial x_n}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1}&\\cdots&\\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\overrightarrow{v}=\n",
    "{\\begin{pmatrix}\n",
    "\\frac{\\partial l}{\\partial y_1}&\\cdots&\\frac{\\partial l}{\\partial y_m}\n",
    "\\end{pmatrix}}^T\n",
    "$$\n",
    "上文中的$\\overrightarrow{v}$即为这里的$\\overrightarrow{v}$。\n",
    "# 三、神经网络\n",
    "PyTorch使用torch.nn包来构建神经网络。\n",
    "## （一）定义神经网络\n",
    "nn.Conv2d：3个数字分别表示卷积层接收的输入数据的个数、卷积层输出数据的个数（即卷积核的个数）、卷积核的维度，stride表示步长，padding默认为0，表示用0填充输入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (f1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (f2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (f3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        #假定输入数据为一张32×32的图片，采样层中每个神经元对上一层中对应特征映射的2×2邻域相连\n",
    "        self.conv1=Conv2d(1,6,(5,5))#1代表单通道，输出数据为6@28×28，采样后为6@14×14\n",
    "        self.conv2=Conv2d(6,16,5)#输出数据为16@10×10，采样后为16@5×5\n",
    "        #映射操作：y=Wx+b\n",
    "        self.f1=Linear(16*5*5,120)#输入数据维度为16×5×5，120个神经元\n",
    "        self.f2=Linear(120,84)\n",
    "        self.f3=Linear(84,10)#最终输出10维向量\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #最大采样层中每个神经元对上一层中对应特征映射的2×2邻域相连\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))#如果池化层的大小是平方的形式，则可使用单个数字替代\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        #特征提取完毕，使用Flatten层“熨平”数据\n",
    "        x=torch.flatten(x,start_dim=1)\n",
    "        x=F.relu(self.f1(x))\n",
    "        x=F.relu(self.f2(x))\n",
    "        x=self.f3(x)\n",
    "        return x\n",
    "\n",
    "net=NeuralNetwork()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过list(net.parameters())[i]查看第i层的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （二）训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0054,  0.0010, -0.1185, -0.1068,  0.0640, -0.0451, -0.0852,  0.1205,\n",
       "          0.0734, -0.0400]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.rand(1,1,32,32)#1张单通道的32×32的图片\n",
    "pred=net(x)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （三）损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5350, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.rand(1,10)\n",
    "y=y.view(1,-1)#view指调整张量的维度，但数据不变，-1表示自动\n",
    "optim=MSELoss()\n",
    "loss=optim(pred,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before backward\n",
      "None\n",
      "after backward\n",
      "tensor([ 0.0134,  0.0006, -0.0057,  0.0095, -0.0160, -0.0031])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "print('before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "print('after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （四）更新权重\n",
    "最简易的权重更新方法是随机剃度下降：\n",
    "$$w_i=w_i-\\eta\\Delta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "for i in net.parameters():\n",
    "    i.data.sub_(lr*i.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch.optim as optim\\noptimizer=optim.SGD(net.parameters(),lr=lr)\\n#放在训练循环中\\noptimizer.zero_grad()\\nloss=loss_fn(pred,y)\\nloss.backward()\\noptimizer.step()\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=0\n",
    "if a==1:\n",
    "    import torch.optim as optim\n",
    "    optimizer=optim.SGD(net.parameters(),lr=lr)\n",
    "    #放在训练循环中\n",
    "    optimizer.zero_grad()\n",
    "    loss=loss_fn(pred,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da5d3c13d5aa829161cf843792884de182f91ab2e540693bb706e0e23cabc9e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
